Goals and Requirements:
----------------------

The goal is to design a parallel domain decomposition MD program 
that shares some components with MolMcD, and lives in the same 
repository. In what follows, I'll tentatively call the new program 
MdDomain.

Eventually, I'd like to be able to use MdDomain in the following 
ways:

1) Use MdDomain by itself to simulate a single system.

2) Use MdDomain by itself to simulate multiple, physically distinct
systems. As a first step, they would not necessarily communicate
with one another in this mode.

2) Use a MdDomain simulation of a single system with a MolMcD 
Simulation object as a master node. 

3) Simulations of multiple systems with a MolMcD Simulation as
a master node for each, and a separate communicator for each
System.

The reason to allow modes in which MolMcD is used as a master 
node is to allow reuse of the code in MolMcD for diagnostics, 
IO, extended ensembles, and replica exchange. MdDomain would 
thus be kept as simple as possible, consistent with the 
requirement that is also possible to run MdDomain by itself. 

To simplify the design of master-slave operation, I would use 
a separate node for the master, even if that node ends up being 
underutilized. Each node would thus create either an appropriate
type of MolMcD Simulation object, or an object for parallel MD
simulation.

The first step would be to design and implement a system that 
can be used in mode 1 (a single system parallel md simulation),
with no master node. 

class MdDomain
---------------

The main class in MdDomain would be a class analogous to 
MdSystem, which I'll tentatively call MdDomain (I'm using
the same name for the program as a whole, and the executable).
This represents the domain owned by one processor. There will 
be no parent Simulation class on a MdDomain processor, so an 
MdDomain object will have to have most of the attributes of
both an MdSystem and a Simulation. 

At a bare minimum, an MdDomain would need to have:

   - An array of objects that represent atoms.

   - An array of objects that represent bonds.

   - A SpeciesManager.

   - A Boundary

   - Potential Energy classes (PairPotential, etc.)

   - a Verlet PairList

The domain decompostion algorithm requires that each 
processor keep track of the coordinates of atoms that 
it owns (i.e., those within the boundaries of the 
asssociated domain) and also "ghost" atoms that are 
owned by other processors by lie within slabs that
are close enough to the boundaries to interact with 
the atoms in that domain. I proposed to store both
types of atoms in a single array, with the local
atoms listed consecutively from the bottom (i.e.,
upwards from index 0) and the ghost atoms listed
downward from the maximum index.  I plan to maintain 
contiguous blocks by an algorithm for removing an atom 
from a processor that moves the last atom in the list
of local atoms and ghosts to the space that the removed 
atom used to occupy, as done now in an ArraySet 
container. This will change the ordering of atoms
every time an atom is removed, but this is fine,
the order in which atoms are stored on each processor 
is irrelevant.

In order to be able to find the local index of an atom,
I plan to maintain one array named something like local_ 
with a dimension equal to the total number of atoms in 
the physical system (on all processors) such that 
local_[globalId] is either a pointer to or the local 
index of an atom with a permanent global index globalId. 

Classes to Describe Atoms, Bonds, etc.
--------------------------------------

The objects that represent atoms will have to be
slightly different than those used in MolMcD, and
will probably have to be represented by a different
class. I'll tenatively call the new atom class PAtom
(the subscript P denotes parallel).  Like an Atom, each 
PAtom will have a Vector position, a typeId and a 
permanent global id.  Unlike MolMcD, however, there 
will be no global array of Molecule objects on each
processor, and so each Atom cannot return a reference 
to its parent molecule. Instead, each atom should be 
able to return an index for a molecule, where the 
index is an index of the molecule within its species, 
and an index to specify the species of molecule. The 
numbering scheme for atoms within molecules will be 
the same as in MolMcD: Each molecule in the physical
system will correspond to a contiguous block of 
(global, permanent) atom ids. The design of PAtom 
could thus be very similar to that of Atom, except 
for the replacement of molecule pointers by molecule 
ids, and the addition of a method to return the 
species number. These changes, however, will be enough 
to force me to define a new class.

In MolMcD, Group objects hold pointers to atoms.  If 
I define a new Atom class, I'll also have to define 
a new Group template. Alternatively, I could define 
group as a template Group< N, AtomType > where AtomType 
could be Atom for MolMcD and PAtom for MdDomain. Similarly,
I may have to define a new PMask class, since the
implemention of PMask uses pointers to atoms, or
make Mask a template in which the atom type is a
template parameter. I need to think through whether
it will be easier to use templates for Group and Mask,
or to just define a new classes PGroup and PMask. I
guess it doesn't matter very much.

I will try to reuse the Species class, and all of 
its subclasses, as descriptions of chemical structure. 
A species in a parallel simulation will have no need
for a reservoir. This may thus require me to either 
move the reservoir out of the Species class, or make
the allocation of the reservoir a separate step that
is not carried out automatically in Species::readParam.

I haven't decided how to keep track of information
about covalent bonds.  If I am willing to initially 
limit the code to systems with bonds (i.e., covalent
bonds that are part of the chemical structure specified 
by Species) but no additional links (bonds that are 
not part of the chemical structure of any well defined 
species, such as crosslinks), then I could use the 
Species class to calculate which of the atoms in a
Domain are connected by bonds. This would require
that every processor have a SpeciesManager. If I
want to be able to include arbitrarily complicated 
bonding topologies, however, such as those generated 
by living polymers or making networks, I'd need to 
pass information about the connectivity of each atom 
between processors when I transfer ownership of an
atom between processors. This is what is done in 
lammps. The advantage of this generality. I'm leaning
towards generality.

Thus far, I see no need for a Molecule class in a 
parallel Md code.

Neighbor and Cell Lists
-----------------------
Each node in a parallel MD code needs a Verlet list 
that keeps track of all pairs involving the atoms that 
it owns. This includes both pairs of local atoms (i.e., 
pairs in which both atoms are owned by the processor) 
and pairs in which one atom is local and the other is 
a ghost that is owned by a neighboring processor. The
construction of this PairList requires a CellList for 
the region containing both the domain owned by the 
processor and the slabs containing ghosts in neighboring 
processors. 

It appears that there are several important differences
between the CellList and PairList will be required by
a parallel MD program and those used now, which may 
make it difficult to use the same classes for both.
The most important difference is that the use of ghost
atoms will make it unnecessary for the CellList and 
PairList that are used by a MdDomain to worry about
periodic boundary conditions. This may make it 
necessary to write new classes.

The question of whether I should try to modify and
reuse the CellList and PairList or write new ones
for this purpose also depends a bit on how I expect
the implementation of these classes to evolve in
the future, and if the expected evolution for the 
serial code is compatible with what I need for
parallel MD.  There are one optimizations that I 
have been considering for the serial code:

In a serial systems with periodic boundary 
conditions and no "ghost" atoms, the implementation
of the minimum image convention should be moved out
of the inner force or energy loop and into the Cell
and/or Pair List. The way to do this is to have the
the Cell and PairLists return not just pointers to 
neighboring atoms, but instances of a "Neighbor"
object in which each Neighbor contains a pointer 
to an atom and (in some form) the shift vector 
necessary to shift the position of the atom to its 
nearest image.  The method of the CellList that 
returns an array of atoms neighboring a particular 
cell would instead return an array of Neighbor 
objects. Similarly, the PairList iterator would 
return a pointer to a primary atom and a Neighbor 
to represent the secondary atom. 

The use of ghost atoms in DomainMD makes the first
optimization unnecessary (wrong, actually) in a
parallel MD code. In order to keep the ability to
add this optimization to the serial code, I guess
I will thus need to write entirely new Cell List
and VerletList classes for MdDomain. This is a bit 
of a maintenance nuisance, but will not necessarily 
so hard to write, since I can just copy the old 
classes and start making modifications. It appears
that this path (the creation of new classes) may
be forced by the different ways of treating 
periodic boundary conditions in the two codes.

Except for the treatment of boundary conditions, the
CellList and PairList classes for parallel MD would
be very similar to the current classes: All of the
local atoms and ghost atoms on a processor would be
added to the CellList. The domain spanned by the 
CellList would include the processor domain and the
surrounding regions occupied by nearby ghosts. Because 
the positions of the ghosts would be the stored as 
nearest image positions, there would be no need to 
worry about periodic boundary conditions within 
either the cell list or the pair list implementation.
In the parallel MD code, periodic boundary conditions
would instead be taken into account during the 
process of updating the ghost positions when the
corresponding local atom positions are updated.
  
Also, in either a serial or parallel Md algorithm, 
there is no need for the ability to rapidly remove 
an atom from a cell. The current Cell List is designed 
to allow rapid deletion as well as rapid addition, 
because it was designed for use in MC as well as Md. 
The algorithm used to do this requires that we keep 
an array of Tag objects with a dimension equal to 
the total number of atoms in the system.  A 
somewhat simpler class might be sufficient, and
slightly more efficient, for MD simulations.

Potential Classes
---------------------------

It appears that the potential energy classes could be 
reused if all of the function calls in MdDomain used
the primitive methods that take Vector arguments, rather 
than those which take Atom or Bond objects as arguments.
The latter set of methods will be unusable in MdDomain
because atoms and bonds will be represented by different
types. 

In order to reduce the level of coupling between classes,
I propose to also rewrite the functions that call the
potential energy classes in MolMcD so as to use the more
primitive methods with Vector arguments, and then remove
the classes that take Atom and Bond objects from the
class definitions. 

I have also been considering seeing if I can speed up the 
inner loop in the serial code by making Boundary and the 
potential energy classes true members of the System class, 
rather than dynamically allocating instances of these 
classes and accessing them through pointers, and then 
rewriting the methods that calculate potential energies 
and forces in McSystem and MdSystem so as to use the 
primitive members of these classes, the ones with Vector 
arguments, as discussed above. The reason I think this 
might speed things up is that allows the compiler to know 
the address of the members of the Boundary and potential
energy classes at compile time (relative to the address
of the main System object), which should make it easier
to calculate more of the required addresses at compile
time when it inlines the methods of the Boundary and
pair potential classes. I'm still a bit confused about
this, but what I've read suggests that it is hard or
impossible for a compiler to take full advantage of
inlining when you ask it to inline a method of an 
object that is accessed through a pointer, but that
it can fully inline the method if the object is 
actually a member of the parent class. I won't know
if there is a real speed advantage until I try this,
but this is another reason to stop using and get 
rid of the "higher level" energy and force calculator
methods.

What can and cannot be Reused?
------------------------------

The above discussion suggests that I will have to write quite 
a few classes that are analogous to those used in MolMcD, but 
different in subtle ways. Among the classes that are specific 
to molecular simulation that will be usable in both codes with 
little or no alteration are:

   - src/math: Vector, IntVector, and Random
   - src/boundary: Boundary
   - src/potential: Potential Energy Classes
   - src/species: Species (if used at all in DomainMD)
  
In addition, however, I will be able to reuse many more general
utility classes that are not specific to simulation applications.
Among these are those in:

   - src/containers:
   - src/format (the C++ stream format classes)
   - src/paramcomposite 
   - src/util (Exception, Log and FileMaster classes)

It appears from the above list that the files that I may be
able to divide the code for the two programs into three sets
of directories corresponding to those used by both programs,
those used only by MolMcD and those used only by MdDomain.
If I keep these in one repository, it might make sense to
separate these into through master directories. I could also
introduce subdivided namespace for the two programs within a
master name space.  The main reason to consider keeping them 
in one repository is to avoid maintaining two copies of 
shared classes such as the containers.

How would the two programs communicate?
-------------------------------------------------
When MolMcD is used as master node for MdDomain, there would
obviously have to be some way for the two programs to
communicate. If we set this up so that each node has either 
a MolMcD Simulation object or MdDomain object, but not both,
the only possible method of communication would be through 
MPI calls. The interfaces through which the two programs 
would communicate would thus not be function interfaces, 
but formats for MPI messages. If we send data in packed 
buffers of heterogeneous elements of primitive int and 
double data, the two programs would not need access to 
any shared header files in order to communicate. Each
program would simply need some additional code to pack 
and send and receive and to receive and unpack data using 
special buffer formats.

In truth, I haven't thought that hard yet about how to
coordinate the two programs. At this point, I think the
higher priority would be to create a parallel Md program
that functions by itself, and only then think more about
coordination.
